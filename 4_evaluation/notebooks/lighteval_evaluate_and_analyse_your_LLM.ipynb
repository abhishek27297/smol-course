{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZVw9f5QYWPL"
      },
      "source": [
        "# lighteval is your AI evaluation library\n",
        "\n",
        "This notebook explores how you can use lighteval to evaluate and compare LLMs.\n",
        "\n",
        "`lighteval` has been around a while and it's a great tool for getting eval score on major benchmarks. It's just been refactored to support being used like a library in Python, which makes it great for comparing models across benchmarks.\n",
        "\n",
        "So let's dig in to some eval scores.\n",
        "\n",
        "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
        "    <h2 style='margin: 0;color:blue'>Exercise: Evaluate Your Own Model</h2>\n",
        "    <p>Now that you've seen how to evaluate models on specific domains, try evaluating a model on a domain that interests you.</p>\n",
        "    <p><b>Difficulty Levels</b></p>\n",
        "    <p>üê¢ Use the existing medical domain tasks but evaluate a different model from the Hugging Face hub</p>\n",
        "    <p>üêï Create a new domain evaluation by selecting different MMLU tasks (e.g., computer science, mathematics, physics)</p>\n",
        "    <p>ü¶Å Create a custom evaluation task using LightEval's task framework and evaluate models on your specific domain</p>\n",
        "</div>\n",
        "\n",
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afW_CLJoPCnF",
        "outputId": "f3bb4e16-490f-4d8a-b725-5ae118677783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch                              2.4.1+cu121\n",
            "torchaudio                         2.4.1+cu121\n",
            "torchsummary                       1.5.1\n",
            "torchvision                        0.19.1+cu121\n"
          ]
        }
      ],
      "source": [
        "!pip install -qqq -U \"torch<2.5\" \"torchvision<2.5\" \"torchaudio<2.5\" --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip list | grep torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8xMnn_cQ1EEi"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq lighteval tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDKs5ShvXw8K"
      },
      "source": [
        "## Setup `lighteval` evaluation\n",
        "\n",
        "We need to setup the evaluation environment and pipeline. Much of this we will disable because we're keeping things in the notebook, but we could also use `push_to_hub` or `push_to_tensorboard`.\n",
        "\n",
        "### `push_to_hub`\n",
        "\n",
        "This is useful if we're evaluating a model and want to persist its evaluation with weights and configuration on the Hugging Face hub.\n",
        "\n",
        "### `push_to_tensorboard`\n",
        "\n",
        "This would be useful if we were building an evaluation tool or script, where we wanted to view results within tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cUebd-z6IWs",
        "outputId": "1b272d8f-358e-4313-8b9a-003f1a1ea28f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:Using default tokenizer.\n",
            "INFO:absl:Using default tokenizer.\n"
          ]
        }
      ],
      "source": [
        "import lighteval\n",
        "import os\n",
        "from datetime import timedelta\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from accelerate import Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "from lighteval.logging.evaluation_tracker import EvaluationTracker\n",
        "# from lighteval.logging.hierarchical_logger import hlog_warn, htrack\n",
        "# from lighteval.models.model_config import create_model_config\n",
        "from lighteval.pipeline import EnvConfig, ParallelismManager, Pipeline, PipelineParameters\n",
        "\n",
        "TOKEN = os.getenv(\"HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_config = EnvConfig(token=TOKEN, cache_dir=\"~/tmp\")"
      ],
      "metadata": {
        "id": "BM1Wjw49qGGn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_tracker = EvaluationTracker(\n",
        "    output_dir=\"~/tmp\",\n",
        "    save_details=False,\n",
        "    push_to_hub=False,\n",
        "    push_to_tensorboard=False,\n",
        "    public=False,\n",
        "    hub_results_org=False,\n",
        ")"
      ],
      "metadata": {
        "id": "Z54h8-XWqIYb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "muikmXNQXgFv"
      },
      "outputs": [],
      "source": [
        "pipeline_params = PipelineParameters(\n",
        "    launcher_type=ParallelismManager.ACCELERATE,\n",
        "    env_config=env_config,\n",
        "    job_id=1,\n",
        "    override_batch_size=1,\n",
        "    num_fewshot_seeds=0,\n",
        "    max_samples=10,\n",
        "    use_chat_template=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsNjwzCtltkA"
      },
      "source": [
        "# Compares models with `lighteval`\n",
        "\n",
        "We are going to compare two small LLMs on a domain. We will use `Qwen2.5-0.5B` and `SmolLM2-360M-Instruct` and we will evaluate them on a medical domain.\n",
        "\n",
        "We can create a domain evaluation from a subset of MMLU evaluations, by defining the evaluation tasks. In lighteval, tasks are described as strings.\n",
        "\n",
        "`{suite}|{task}:{subtask}|{num_few_shot}|{0 or 1 to reduce num_few_shot if prompt is too long}`\n",
        "\n",
        "Therefore, we will pass our list of medicine related tasks like this:\n",
        "\n",
        "```\n",
        "\"leaderboard|mmlu:anatomy|5|0,leaderboard|mmlu:professional_medicine|5|0,leaderboard|mmlu:high_school_biology|5|0,leaderboard|mmlu:high_school_chemistry|5|0\"\n",
        "```\n",
        "\n",
        "Which can be translated to :\n",
        "\n",
        "| Suite | Task | Num Fewshot Example | Limit Fewshots |\n",
        "|---|---|---|---|\n",
        "| leaderboard | mmlu:anatomy | 5 | False |\n",
        "| leaderboard | mmlu:professional_medicine | 5 | False |\n",
        "| leaderboard | mmlu:high_school_biology | 5 | False |\n",
        "| leaderboard | mmlu:high_school_chemistry | 5 | False |\n",
        "\n",
        "For a full list of lighteval supported tasks. Checkout this page in [the documentation](https://github.com/huggingface/lighteval/wiki/Available-Tasks)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qTqsizv9mVbO"
      },
      "outputs": [],
      "source": [
        "domain_tasks = \"leaderboard|mmlu:anatomy|5|0,leaderboard|mmlu:professional_medicine|5|0,leaderboard|mmlu:high_school_biology|5|0,leaderboard|mmlu:high_school_chemistry|5|0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwcJklSFX4H6"
      },
      "source": [
        "# Evaluate Qwen2.5 0.5B"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qwen_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
        "# qwen_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")"
      ],
      "metadata": {
        "id": "6xtNTnFnrXng"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelInfo:\n",
        "    def __init__(self, model_name, model_sha, model_dtype, model_size):\n",
        "        self.model_name = model_name\n",
        "        self.model_sha = model_sha\n",
        "        self.model_dtype = model_dtype\n",
        "        self.model_size = model_size\n",
        "\n",
        "class QwenModelWrapper:\n",
        "    def __init__(self, model_name_or_path: str):\n",
        "        from transformers import AutoModelForCausalLM\n",
        "\n",
        "        self._model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
        "\n",
        "        self.model_info = ModelInfo(\n",
        "            model_name=model_name_or_path,\n",
        "            model_sha=\"unknown_sha\",\n",
        "            model_dtype=\"float16\",\n",
        "            model_size=\"0.5B\"\n",
        "        )\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self._model(*args, **kwargs)\n",
        "\n",
        "    def generate(self, *args, **kwargs):\n",
        "        return self._model.generate(*args, **kwargs)\n",
        "\n",
        "    def tok_encode(self, text: str):\n",
        "        return self._tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    def tok_decode(self, token_ids: list[int]):\n",
        "        return self._tokenizer.decode(token_ids, skip_special_tokens=True)\n",
        "\n",
        "qwen_model_wrapped = QwenModelWrapper(\"Qwen/Qwen2.5-0.5B\")"
      ],
      "metadata": {
        "id": "imfDcwMA1AWI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(\n",
        "    tasks=domain_tasks,\n",
        "    pipeline_parameters=pipeline_params,\n",
        "    evaluation_tracker=evaluation_tracker,\n",
        "    model=qwen_model_wrapped\n",
        ")"
      ],
      "metadata": {
        "id": "tOIKR8DyrbEr",
        "outputId": "26999c4d-6afd-4c98-b321-4951215725f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:lighteval.logging.hierarchical_logger:WARNING: --max_samples WAS SET. THESE NUMBERS ARE ONLY PARTIAL AND SHOULD NOT BE USED FOR COMPARISON UNLESS YOU KNOW WHAT YOU ARE DOING.\n",
            "WARNING:lighteval.logging.hierarchical_logger:Test all gather {\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Test gather tensor\n",
            "WARNING:lighteval.logging.hierarchical_logger:  gathered_tensor tensor([0], device='cuda:0'), should be [0]\n",
            "WARNING:lighteval.logging.hierarchical_logger:} [0:00:00.408565]\n",
            "WARNING:lighteval.logging.hierarchical_logger:Model loading {\n",
            "WARNING:lighteval.logging.hierarchical_logger:} [0:00:00.000005]\n",
            "WARNING:lighteval.logging.hierarchical_logger:Tasks loading {\n",
            "WARNING:lighteval.logging.hierarchical_logger:  \u001b[33mIf you want to use extended_tasks, make sure you installed their dependencies using `pip install -e .[extended_tasks]`.\u001b[0m\n",
            "WARNING:lighteval.logging.hierarchical_logger:  lighteval/mmlu anatomy\n",
            "WARNING:lighteval.logging.hierarchical_logger:  lighteval/mmlu high_school_biology\n",
            "WARNING:lighteval.logging.hierarchical_logger:  lighteval/mmlu high_school_chemistry\n",
            "WARNING:lighteval.logging.hierarchical_logger:  lighteval/mmlu professional_medicine\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Loading documents, and requests\n",
            "WARNING:lighteval.logging.hierarchical_logger:} [0:00:07.845104]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'QwenModelWrapper' object has no attribute '_tokenizer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-805a3dfe07c5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m pipeline = Pipeline(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdomain_tasks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpipeline_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mevaluation_tracker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluation_tracker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqwen_model_wrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lighteval/pipeline.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tasks, pipeline_parameters, evaluation_tracker, model_config, model)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneral_config_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_model_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_tasks_and_requests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_random_seeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# Final results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lighteval/pipeline.py\u001b[0m in \u001b[0;36m_init_tasks_and_requests\u001b[0;34m(self, tasks)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mhlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading documents, and requests\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 requests, docs = create_requests_from_tasks(\n\u001b[0m\u001b[1;32m    186\u001b[0m                     \u001b[0mtask_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0mfewshot_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfewshots_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lighteval/tasks/lighteval_task.py\u001b[0m in \u001b[0;36mcreate_requests_from_tasks\u001b[0;34m(task_dict, fewshot_dict, num_fewshot_seeds, lm, max_samples, evaluation_tracker, use_chat_template, system_prompt)\u001b[0m\n\u001b[1;32m    657\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mnum_fewshot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate_few_shots\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfewshot_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m                     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m                     doc = prompt_manager.add_context_to_doc(\n\u001b[0m\u001b[1;32m    660\u001b[0m                         \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m                         \u001b[0mnum_fewshot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_fewshot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lighteval/tasks/prompt_manager.py\u001b[0m in \u001b[0;36madd_context_to_doc\u001b[0;34m(self, doc, num_fewshot, seed, sampler, truncate_few_shots, use_chat_template, system_prompt)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecific\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"multi_turn_queries_context\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             ctx, num_effective_few_shots = self._single_turn_context(\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0mdoc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mnum_fewshot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_fewshot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lighteval/tasks/prompt_manager.py\u001b[0m in \u001b[0;36m_single_turn_context\u001b[0;34m(self, doc, num_fewshot, seed, sampler, truncate_few_shots, use_chat_template, system_prompt)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0muse_chat_template\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_chat_template\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mtoks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m# If we need to truncate few-shots to fit in the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-75a6a4a0d1af>\u001b[0m in \u001b[0;36mtok_encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtok_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtok_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'QwenModelWrapper' object has no attribute '_tokenizer'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.evaluate()"
      ],
      "metadata": {
        "id": "F5Ssb7_F10qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXJuaXVxUNBO"
      },
      "outputs": [],
      "source": [
        "qwen_results = pipeline.get_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIwCaCxJX_hA"
      },
      "source": [
        "# Evaluate SmolLM 360M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Dxg0RtlNVT4y"
      },
      "outputs": [],
      "source": [
        "smol_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(\n",
        "    tasks=domain_tasks,\n",
        "    pipeline_parameters=pipeline_params,\n",
        "    evaluation_tracker=evaluation_tracker,\n",
        "    model=smol_model\n",
        ")"
      ],
      "metadata": {
        "id": "hbciWHQRsyR7",
        "outputId": "8ce16925-41d1-436c-b935-ed259cabe05d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:lighteval.logging.hierarchical_logger:WARNING: --max_samples WAS SET. THESE NUMBERS ARE ONLY PARTIAL AND SHOULD NOT BE USED FOR COMPARISON UNLESS YOU KNOW WHAT YOU ARE DOING.\n",
            "WARNING:lighteval.logging.hierarchical_logger:Test all gather {\n",
            "WARNING:lighteval.logging.hierarchical_logger:  Test gather tensor\n",
            "WARNING:lighteval.logging.hierarchical_logger:  gathered_tensor tensor([0], device='cuda:0'), should be [0]\n",
            "WARNING:lighteval.logging.hierarchical_logger:} [0:00:00.009077]\n",
            "WARNING:lighteval.logging.hierarchical_logger:Model loading {\n",
            "WARNING:lighteval.logging.hierarchical_logger:} [0:00:00.000003]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'LlamaForCausalLM' object has no attribute 'model_info'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-36a9dd4db85c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m pipeline = Pipeline(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdomain_tasks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpipeline_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mevaluation_tracker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluation_tracker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msmol_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/lighteval/pipeline.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tasks, pipeline_parameters, evaluation_tracker, model_config, model)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneral_config_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_model_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_tasks_and_requests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_random_seeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1729\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'model_info'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.evaluate()"
      ],
      "metadata": {
        "id": "vUWX-dVAs0cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdjyzfKHVt52"
      },
      "outputs": [],
      "source": [
        "smol_results = pipeline.get_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eugvMFfgV1VD"
      },
      "outputs": [],
      "source": [
        "pipeline.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HD8aFwSYGHu"
      },
      "source": [
        "# Visualize Results\n",
        "\n",
        "Now that we have results from the two models we can visualize them side-by-side. We'll keep visualisation simple here, but with this data structure you could represent scores in many ways."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sReqrgQUO9r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame.from_records(smol_results[\"results\"]).T[\"acc\"].rename(\"SmolLM2-360M-Instruct\")\n",
        "_df = pd.DataFrame.from_records(qwen_results[\"results\"]).T[\"acc\"].rename(\"Qwen2-0.5B-DPO\")\n",
        "df = pd.concat([df, _df], axis=1)\n",
        "df.plot(kind=\"barh\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJEbQeYDplKX"
      },
      "source": [
        "# üíê That's it!\n",
        "\n",
        "You have a handy notebook to view model evals. You could use this to:\n",
        "\n",
        "- select the right model for your inference use case\n",
        "- evaluate checkpoints during training\n",
        "- share model scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWdS38syaipm"
      },
      "source": [
        "üèÉNext Steps\n",
        "\n",
        "- If you want to go deeper into your evaluation results check out this [notebook](https://github.com/huggingface/evaluation-guidebook/blob/main/contents/examples/comparing_task_formulations.ipynb)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}