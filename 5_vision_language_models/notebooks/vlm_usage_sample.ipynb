{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxO5s47By8sC"
      },
      "source": [
        "# Processing images and text with VLMs\n",
        "\n",
        "This notebook demonstrates how to utilize the `HuggingFaceTB/SmolVLM-Instruct` 4bit-quantized model for various multimodal tasks such as:\n",
        "- Visual Question Answering (VQA): Answering questions based on image content.\n",
        "- Text Recognition (OCR): Extracting and interpreting text in images.\n",
        "- Video Understanding: Describing videos through sequential frame analysis.\n",
        "\n",
        "By structuring prompts effectively, you can leverage the model for many applications, such as scene understanding, document analysis, and dynamic visual reasoning."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the requirements in Google Colab\n",
        "!pip install transformers datasets trl huggingface_hub bitsandbytes"
      ],
      "metadata": {
        "id": "Fo11auJpzBwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Qt_Ec8apy8sE"
      },
      "outputs": [],
      "source": [
        "# Authenticate to Hugging Face\n",
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(token=userdata.get('HF_TOKEN'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, PIL\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq, BitsAndBytesConfig\n",
        "from transformers.image_utils import load_image"
      ],
      "metadata": {
        "id": "Tvds1DfkzlGp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        ")\n",
        "device"
      ],
      "metadata": {
        "id": "FC_r79aYzqXR",
        "outputId": "e0cedaad-2cdb-4160-c9a6-674817fdeaec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1N7CRD3cy8sE",
        "outputId": "4d560e2d-6bca-4d06-e819-076df810eeac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
            "Some kwargs in processor config are unused and will not have any effect: image_seq_len. \n"
          ]
        }
      ],
      "source": [
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "model_name = \"HuggingFaceTB/SmolVLM-Instruct\"\n",
        "model = AutoModelForVision2Seq.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        ").to(device)\n",
        "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(processor.image_processor.size)"
      ],
      "metadata": {
        "id": "RxAabUVTzw0Z",
        "outputId": "7767cfb4-c57a-4cce-a5ac-fd7cccc77900",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'longest_edge': 1536}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqYiykWNy8sE"
      },
      "source": [
        "## Processing Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JDZJS8ky8sF"
      },
      "source": [
        "Let's start with generating captions and answering questions about an image. We'll also explore processing multiple images.\n",
        "### 1. Single Image Captioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Nff6vaniy8sF",
        "outputId": "b6e9783f-92ff-4677-9bfb-7e9228df1ddd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://cdn.pixabay.com/photo/2024/11/20/09/14/christmas-9210799_1280.jpg\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://cdn.pixabay.com/photo/2024/11/23/08/18/christmas-9218404_1280.jpg\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "image_url1 = \"https://cdn.pixabay.com/photo/2024/11/20/09/14/christmas-9210799_1280.jpg\"\n",
        "display(Image(url=image_url1))\n",
        "\n",
        "image_url2 = \"https://cdn.pixabay.com/photo/2024/11/23/08/18/christmas-9218404_1280.jpg\"\n",
        "display(Image(url=image_url2))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load  one image\n",
        "image1 = load_image(image_url1)"
      ],
      "metadata": {
        "id": "5vSSRy1o0cQ0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input messages\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": \"Can you describe the image?\"}\n",
        "        ]\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "Ve7m_RMV0iRk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare inputs\n",
        "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = processor(text=prompt, images=[image1], return_tensors=\"pt\")\n",
        "inputs = inputs.to(device)"
      ],
      "metadata": {
        "id": "fZ5x-JjS0nFq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "id": "LjyhPAmy0sNX",
        "outputId": "8f86f057-79de-4167-f99e-35352a89dccb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|im_start|>User:<image>Can you describe the image?<end_of_utterance>\\nAssistant:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate outputs\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
        "generated_texts = processor.batch_decode(\n",
        "    generated_ids,\n",
        "    skip_special_tokens=True\n",
        ")"
      ],
      "metadata": {
        "id": "WQYmDOBE057m",
        "outputId": "bb854dc0-0d66-4609-9e76-9d6a85b06586",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kG3HhRJoy8sF",
        "outputId": "a4510544-93a2-4e99-9562-a1842adb74fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['User:<image>Can you describe the image?\\nAssistant: The image is a scene of a person walking in a forest. The person is wearing a coat and a cap. The person is holding the hand of another person. The person is walking on a path. The path is covered with dry leaves. The background of the image is a forest with trees.']\n"
          ]
        }
      ],
      "source": [
        "print(generated_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyF4IE4ry8sF"
      },
      "source": [
        "### 2. Comparing Multiple Images\n",
        "The model can process and compare multiple images. Let's determine the common theme between two images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "hA7x2jcxy8sF",
        "outputId": "fa64ac14-9e0a-4f61-af3e-fd3e0f9bd0b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['User:<image>What event do they both represent?\\nAssistant: Christmas.']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load images\n",
        "image2 = load_image(image_url2)\n",
        "\n",
        "# Create input messages\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": \"What event do they both represent?\"}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "# Prepare inputs\n",
        "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = processor(text=prompt, images=[image1, image2], return_tensors=\"pt\")\n",
        "inputs = inputs.to(device)\n",
        "\n",
        "# Generate outputs\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
        "generated_texts = processor.batch_decode(\n",
        "    generated_ids,\n",
        "    skip_special_tokens=True,\n",
        ")\n",
        "\n",
        "print(generated_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCVxB8wiy8sF"
      },
      "source": [
        "### üî† Text Recognition (OCR)\n",
        "VLM can also recognize and interpret text in images, making it suitable for tasks like document analysis.\n",
        "You could try experimenting on images with denser text."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_image_url = \"https://cdn.pixabay.com/photo/2020/11/30/19/23/christmas-5792015_960_720.png\"\n",
        "display(Image(url=document_image_url))"
      ],
      "metadata": {
        "id": "Z5K0IwFj1Lvu",
        "outputId": "18c73825-cf06-4318-95a0-ba17cbcc795b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://cdn.pixabay.com/photo/2020/11/30/19/23/christmas-5792015_960_720.png\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the document image\n",
        "document_image = load_image(document_image_url)"
      ],
      "metadata": {
        "id": "9rDS_nNg1SPX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input message for analysis\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": \"What is written?\"}\n",
        "        ]\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "d6Ul3WsM1U_X"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare inputs\n",
        "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "print(prompt)\n",
        "inputs = processor(text=prompt, images=[document_image], return_tensors=\"pt\")\n",
        "inputs = inputs.to(device)"
      ],
      "metadata": {
        "id": "saM9NKsX1XLY",
        "outputId": "eb1f6ecc-22c4-4d50-bf10-80f002cad4df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|im_start|>User:<image>What is written?<end_of_utterance>\n",
            "Assistant:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate outputs\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
        "generated_texts = processor.batch_decode(\n",
        "    generated_ids,\n",
        "    skip_special_tokens=True\n",
        ")"
      ],
      "metadata": {
        "id": "o-27XZ231fiD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "H8LDWCPty8sG",
        "outputId": "b146581e-61fd-4977-b7da-ca229c9c964b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['User:<image>What is written?\\nAssistant: MERRY CHRISTMAS AND A HAPPY NEW YEAR.']\n"
          ]
        }
      ],
      "source": [
        "print(generated_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOtMPTsEy8sG"
      },
      "source": [
        "## Processing videos\n",
        "\n",
        "Visual-Language Models (VLMs) can process videos indirectly by extracting keyframes and reasoning over them in temporal order. While VLMs lack the temporal modeling capabilities of dedicated video models, they can still:\n",
        "- Describe actions or events by analyzing sampled frames sequentially.\n",
        "- Answer questions about videos based on representative keyframes.\n",
        "- Summarize video content by combining textual descriptions of multiple frames.\n",
        "\n",
        "Let experiment on an example:\n",
        "\n",
        "<video width=\"640\" height=\"360\" controls>\n",
        "  <source src=\"https://cdn.pixabay.com/video/2023/10/28/186794-879050032_large.mp4\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "</video>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvRnqGTmy8sG"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Video\n",
        "import cv2\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "_k3aqSdi2fsN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "S6JAql7sy8sG"
      },
      "outputs": [],
      "source": [
        "def extract_frames(video_path, max_frames=50, target_size=None):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"Could not open video: {video_path}\")\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_indices = np.linspace(0, total_frames - 1, max_frames, dtype=int)\n",
        "\n",
        "    frames = []\n",
        "    for idx in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frame = PIL.Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "            if target_size:\n",
        "                frames.append(resize_and_crop(frame, target_size))\n",
        "            else:\n",
        "                frames.append(frame)\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def resize_and_crop(image, target_size):\n",
        "    width, height = image.size\n",
        "    scale = target_size / min(width, height)\n",
        "    image = image.resize((int(width * scale), int(height * scale)), PIL.Image.Resampling.LANCZOS)\n",
        "    left = (image.width - target_size) // 2\n",
        "    top = (image.height - target_size) // 2\n",
        "    return image.crop((left, top, left + target_size, top + target_size))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Video link\n",
        "video_link = \"https://cdn.pixabay.com/video/2023/10/28/186794-879050032_large.mp4\""
      ],
      "metadata": {
        "id": "wQHimrCM2qCH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(model, processor, frames, question):\n",
        "\n",
        "    image_tokens = [{\"type\": \"image\"} for _ in frames]\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": \"Following are the frames of a video in temporal order.\"}, *image_tokens, {\"type\": \"text\", \"text\": question}]\n",
        "        }\n",
        "    ]\n",
        "    inputs = processor(\n",
        "        text=processor.apply_chat_template(messages, add_generation_prompt=True),\n",
        "        images=frames,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs, max_new_tokens=100, num_beams=5, temperature=0.7, do_sample=True, use_cache=True\n",
        "    )\n",
        "    return processor.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "epU-31l82yga"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Describe what the woman is doing.\""
      ],
      "metadata": {
        "id": "_AS_C7cx5G99"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract frames from the video\n",
        "frames = extract_frames(video_link, max_frames=15, target_size=384)\n",
        "frames"
      ],
      "metadata": {
        "id": "NLWINYCi5IVX",
        "outputId": "84d64ca5-45dc-494f-de5c-6420ef6030f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<PIL.Image.Image image mode=RGB size=384x384>,\n",
              " <PIL.Image.Image image mode=RGB size=384x384>,\n",
              " <PIL.Image.Image image mode=RGB size=384x384>,\n",
              " <PIL.Image.Image image mode=RGB size=384x384>,\n",
              " <PIL.Image.Image image mode=RGB size=384x384>,\n",
              " <PIL.Image.Image image mode=RGB size=384x384>,\n",
              " <PIL.Image.Image image mode=RGB size=384x384>,\n",
              " <PIL.Image.Image image mode=RGB size=384x384>,\n",
              " <PIL.Image.Image image mode=RGB size=384x384>,\n",
              " <PIL.Image.Image image mode=RGB size=384x384>,\n",
              " <PIL.Image.Image image mode=RGB size=384x384>,\n",
              " <PIL.Image.Image image mode=RGB size=384x384>,\n",
              " <PIL.Image.Image image mode=RGB size=384x384>,\n",
              " <PIL.Image.Image image mode=RGB size=384x384>,\n",
              " <PIL.Image.Image image mode=RGB size=384x384>]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BVbWHuLuy8sG"
      },
      "outputs": [],
      "source": [
        "processor.image_processor.size = (384, 384)\n",
        "processor.image_processor.do_resize = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate response\n",
        "response = generate_response(model, processor, frames, question)"
      ],
      "metadata": {
        "id": "YA9BZAe43FTo",
        "outputId": "d8ac7ced-11ab-4f42-c867-bd46dca8ff21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the result\n",
        "# print(\"Question:\", question)\n",
        "print(\"Response:\", response)"
      ],
      "metadata": {
        "id": "tySLEw2t3ETs",
        "outputId": "61845b33-0c0d-469a-858d-fcf3291ce030",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: User: Following are the frames of a video in temporal order.<image>Describe what the woman is doing.\n",
            "Assistant: The woman is hanging an ornament on a Christmas tree.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttNsb3Isy8sG"
      },
      "source": [
        "## üíê You're Done!\n",
        "\n",
        "This notebook demonstrated how to use a Visual-Language Model (VLM) such as formating prompts for multimodal tasks. By following the steps outlined here, you can experiment with VLMs and their applications.\n",
        "\n",
        "### Next Steps to Explore:\n",
        "- Experiment with more use cases of VLMs.\n",
        "- Collaborate with a colleague by reviewing their pull requests (PRs).\n",
        "- Contribute to improving this course material by opening an issue or submitting a PR to introduce new use cases, examples, or concepts.\n",
        "\n",
        "Happy exploring! üåü"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}